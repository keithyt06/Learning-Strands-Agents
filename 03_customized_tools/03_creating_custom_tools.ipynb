{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第3章：Strands Agents中的自定义工具\n",
    "\n",
    "## 自定义工具简介\n",
    "\n",
    "虽然内置工具为许多任务提供了坚实的基础，但Strands Agents的真正力量来自于能够创建适合您特定需求的自定义工具。在本章中，我们将探讨如何创建、完善和扩展您自己的工具，以增强代理的能力。\n",
    "\n",
    "自定义工具允许您：\n",
    "\n",
    "- 将代理连接到您自己的API和服务\n",
    "\n",
    "- 创建特定领域的功能\n",
    "\n",
    "- 构建专门的数据处理工具\n",
    "\n",
    "- 与数据库和外部系统对接\n",
    "\n",
    "- 实现特定于您的用例的业务逻辑\n",
    "\n",
    "在本章中，我们将按照课程要求使用Claude 3.7 Sonnet模型（`us.anthropic.claude-3-7-sonnet-20250219-v1:0`）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置和前提条件\n",
    "\n",
    "让我们从安装必要的包开始："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: strands-agents in /opt/conda/lib/python3.11/site-packages (0.1.6)\n",
      "Requirement already satisfied: strands-agents-tools in /opt/conda/lib/python3.11/site-packages (0.1.4)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from strands-agents) (1.34.162)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.29.0 in /opt/conda/lib/python3.11/site-packages (from strands-agents) (1.34.162)\n",
      "Requirement already satisfied: docstring-parser<0.16.0,>=0.15 in /opt/conda/lib/python3.11/site-packages (from strands-agents) (0.15)\n",
      "Requirement already satisfied: mcp<2.0.0,>=1.8.0 in /opt/conda/lib/python3.11/site-packages (from strands-agents) (1.9.2)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.30.0 in /opt/conda/lib/python3.11/site-packages (from strands-agents) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.30.0 in /opt/conda/lib/python3.11/site-packages (from strands-agents) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.30.0 in /opt/conda/lib/python3.11/site-packages (from strands-agents) (1.33.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from strands-agents) (2.10.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.13.2 in /opt/conda/lib/python3.11/site-packages (from strands-agents) (4.13.2)\n",
      "Requirement already satisfied: watchdog<7.0.0,>=6.0.0 in /opt/conda/lib/python3.11/site-packages (from strands-agents) (6.0.0)\n",
      "Requirement already satisfied: aws-requests-auth<0.5.0,>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from strands-agents-tools) (0.4.3)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.6 in /opt/conda/lib/python3.11/site-packages (from strands-agents-tools) (0.4.6)\n",
      "Requirement already satisfied: dill<0.5.0,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from strands-agents-tools) (0.4.0)\n",
      "Requirement already satisfied: pillow<12.0.0,>=11.2.1 in /opt/conda/lib/python3.11/site-packages (from strands-agents-tools) (11.2.1)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.51 in /opt/conda/lib/python3.11/site-packages (from strands-agents-tools) (3.0.51)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.10.1 in /opt/conda/lib/python3.11/site-packages (from strands-agents-tools) (2.10.1)\n",
      "Requirement already satisfied: rich<15.0.0,>=14.0.0 in /opt/conda/lib/python3.11/site-packages (from strands-agents-tools) (14.0.0)\n",
      "Requirement already satisfied: slack-bolt<2.0.0,>=1.23.0 in /opt/conda/lib/python3.11/site-packages (from strands-agents-tools) (1.23.0)\n",
      "Requirement already satisfied: sympy<2.0.0,>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from strands-agents-tools) (1.13.3)\n",
      "Requirement already satisfied: tenacity<10.0.0,>=9.1.2 in /opt/conda/lib/python3.11/site-packages (from strands-agents-tools) (9.1.2)\n",
      "Requirement already satisfied: requests>=0.14.0 in /opt/conda/lib/python3.11/site-packages (from aws-requests-auth<0.5.0,>=0.4.3->strands-agents-tools) (2.32.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents) (0.10.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents) (1.26.19)\n",
      "Requirement already satisfied: anyio>=4.5 in /opt/conda/lib/python3.11/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents) (4.7.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /opt/conda/lib/python3.11/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents) (0.4.0)\n",
      "Requirement already satisfied: httpx>=0.27 in /opt/conda/lib/python3.11/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents) (0.28.1)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /opt/conda/lib/python3.11/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents) (2.9.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /opt/conda/lib/python3.11/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents) (0.0.19)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /opt/conda/lib/python3.11/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents) (2.3.6)\n",
      "Requirement already satisfied: starlette>=0.27 in /opt/conda/lib/python3.11/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents) (0.41.3)\n",
      "Requirement already satisfied: uvicorn>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents) (0.32.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-api<2.0.0,>=1.30.0->strands-agents) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-api<2.0.0,>=1.30.0->strands-agents) (6.10.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.30.0->strands-agents) (1.66.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.33.1 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.30.0->strands-agents) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.33.1 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.30.0->strands-agents) (1.33.1)\n",
      "Requirement already satisfied: protobuf<6.0,>=5.0 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-proto==1.33.1->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.30.0->strands-agents) (5.29.5)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.54b1 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-sdk<2.0.0,>=1.30.0->strands-agents) (0.54b1)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit<4.0.0,>=3.0.51->strands-agents-tools) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->strands-agents) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->strands-agents) (2.27.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich<15.0.0,>=14.0.0->strands-agents-tools) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich<15.0.0,>=14.0.0->strands-agents-tools) (2.18.0)\n",
      "Requirement already satisfied: slack_sdk<4,>=3.35.0 in /opt/conda/lib/python3.11/site-packages (from slack-bolt<2.0.0,>=1.23.0->strands-agents-tools) (3.35.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy<2.0.0,>=1.12.0->strands-agents-tools) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.8.0->strands-agents) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.8.0->strands-agents) (1.3.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.11/site-packages (from deprecated>=1.2.6->opentelemetry-api<2.0.0,>=1.30.0->strands-agents) (1.17.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx>=0.27->mcp<2.0.0,>=1.8.0->strands-agents) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx>=0.27->mcp<2.0.0,>=1.8.0->strands-agents) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27->mcp<2.0.0,>=1.8.0->strands-agents) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api<2.0.0,>=1.30.0->strands-agents) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=14.0.0->strands-agents-tools) (0.1.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from pydantic-settings>=2.5.2->mcp<2.0.0,>=1.8.0->strands-agents) (1.0.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic-settings>=2.5.2->mcp<2.0.0,>=1.8.0->strands-agents) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.29.0->strands-agents) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=0.14.0->aws-requests-auth<0.5.0,>=0.4.3->strands-agents-tools) (3.4.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.11/site-packages (from uvicorn>=0.23.1->mcp<2.0.0,>=1.8.0->strands-agents) (8.1.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install strands-agents and strands-agents-tools if you haven't already\n",
    "%pip install -U strands-agents strands-agents-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS credentials configured correctly!\n"
     ]
    }
   ],
   "source": [
    "# Import the AWS SDK for Python\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# Option 1: Set environment variables (uncomment and set your values)\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = '...'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = '...'\n",
    "os.environ['AWS_REGION'] = '...'\n",
    "# Option 2: Create a boto3 session with your credentials\n",
    "# session = boto3.Session(\n",
    "#     aws_access_key_id='your_access_key_id',\n",
    "#     aws_secret_access_key='your_secret_access_key',\n",
    "#     region_name='us-west-2'  # or your preferred region\n",
    "# )\n",
    "\n",
    "# Verify your configuration\n",
    "try:\n",
    "    boto3.client('bedrock-runtime')\n",
    "    print(\"AWS credentials configured correctly!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring AWS credentials: {e}\")\n",
    "    print(\"Please set your AWS credentials before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建你的第一个自定义Tool\n",
    "\n",
    "在Strands Agents中创建自定义工具最简单的方法是使用`@tool`装饰器与Python函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent, tool\n",
    "\n",
    "# Define a simple custom tool\n",
    "@tool\n",
    "def word_counter(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    统计文本中每个单词的出现次数。\n",
    "参数：\n",
    "text (str)：要分析的输入文本\n",
    "返回：\n",
    "dict：一个将单词映射到其出现次数的字典\n",
    "    \"\"\"\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    import string\n",
    "    text = text.lower()\n",
    "    for punct in string.punctuation:\n",
    "        text = text.replace(punct, ' ')\n",
    "    \n",
    "    # Split into words and count occurrences\n",
    "    words = text.split()\n",
    "    word_counts = {}\n",
    "    for word in words:\n",
    "        word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    \n",
    "    return word_counts\n",
    "\n",
    "# Create an agent with our custom tool\n",
    "agent = Agent(\n",
    "    model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    tools=[word_counter],\n",
    "    system_prompt=\"你是一个能够分析文本并提供词语统计的助手。\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our custom tool by asking the agent to analyze some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我可以帮你分析这段文本并找出出现频率最高的词。让我使用word_counter工具来统计每个词的出现次数。\n",
      "Tool #1: word_counter\n",
      "我注意到词语统计结果似乎将整个文本分成了几个大块，而不是按照单个词进行统计。这可能是因为中文文本的分词处理方式不同。\n",
      "\n",
      "让我重新尝试，将文本中的英文部分与中文分开处理：\n",
      "Tool #2: word_counter\n",
      "分析结果显示，在这段文本中出现频率最高的词语是：\n",
      "\n",
      "1. \"的\" - 出现了5次\n",
      "2. \"代理\" - 出现了3次\n",
      "5. \"。\" (句号) - 出现了3次\n",
      "3. \"Strands\" - 出现了2次\n",
      "4. \"Agents\" - 出现了2次\n",
      "5. \"构建\" - 出现了2次\n",
      "6. \"AI\" - 出现了2次\n",
      "7. \"框架\" - 出现了2次\n",
      "8. \"，\"(逗号) - 出现了2次\n",
      "9. \"你\" - 出现了2次\n",
      "10. \"复杂\" - 出现了2次\n",
      "\n",
      "如果我们忽略标点符号，那么\"的\"和\"代理\"是这段文本中出现最频繁的词语。这反映了这段文本主要讨论的是关于构建AI代理的内容。"
     ]
    }
   ],
   "source": [
    "response = agent(\"\"\"\n",
    "请分析以下段落并告诉我哪些词出现频率最高：\n",
    "\"Strands Agents是一个使用Python构建AI代理的强大框架。\n",
    "通过Strands Agents，你可以创建利用Nova Pro等语言模型\n",
    "来执行复杂任务的智能代理。该框架提供了增强\n",
    "你的代理各种能力的工具，使构建复杂的AI\n",
    "应用变得简单。\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义工具的解析\n",
    "让我们分解Strands Agents中自定义工具的关键组成部分：\n",
    "1. **`@tool`装饰器**：这告诉Strands该函数应被视为工具\n",
    "2. **Type注解**：用于定义预期的输入和输出类型\n",
    "3. **文档字符串**：对于代理理解何时以及如何使用工具至关重要\n",
    "4. **实现**：工具被使用时实际执行的代码\n",
    "\n",
    "### 良好文档字符串的重要性\n",
    "\n",
    "文档字符串特别重要，因为它帮助AI模型理解：\n",
    "\n",
    "- 工具的功能\n",
    "- 何时使用它\n",
    "- 它期望的输入\n",
    "- 它产生的输出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def text_sentiment_analyzer(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    使用简单的基于词典的方法分析文本情感。\n",
    "这个工具通过计算预定义词典中的积极和消极词汇来对文本情感进行分类。它适用于快速评估用户反馈、产品评论或社交媒体评论的情感。\n",
    "\n",
    "参数：\n",
    "text (str)：要分析情感的文本。应该是英文，最好包含至少10个单词以获得更好的准确性。\n",
    "\n",
    "返回：\n",
    "dict：包含以下内容的字典：\n",
    "- 'sentiment'：整体情感（'positive'、'negative'或'neutral'）\n",
    "- 'score'：情感分数，从-1.0（非常消极）到1.0（非常积极）\n",
    "- 'positive_words'：文本中发现的积极词汇列表\n",
    "- 'negative_words'：文本中发现的消极词汇列表\n",
    "\n",
    "注意：这是一个用于演示目的的简化情感分析器。\n",
    "    \"\"\"\n",
    "    # Simple lexicons\n",
    "    positive_words = {\n",
    "        'good', 'great', 'excellent', 'wonderful', 'fantastic', 'amazing', 'love', 'best',\n",
    "        'happy', 'joy', 'positive', 'helpful', 'beautiful', 'perfect', 'recommend'\n",
    "    }\n",
    "    \n",
    "    negative_words = {\n",
    "        'bad', 'terrible', 'awful', 'horrible', 'poor', 'worst', 'hate', 'dislike', \n",
    "        'negative', 'disappointed', 'disappointing', 'useless', 'waste', 'broken'\n",
    "    }\n",
    "    \n",
    "    # Normalize text\n",
    "    text = text.lower()\n",
    "    words = ''.join(c if c.isalnum() else ' ' for c in text).split()\n",
    "    \n",
    "    # Find positive and negative words\n",
    "    found_positive_words = [word for word in words if word in positive_words]\n",
    "    found_negative_words = [word for word in words if word in negative_words]\n",
    "    \n",
    "    # Calculate sentiment score\n",
    "    pos_count = len(found_positive_words)\n",
    "    neg_count = len(found_negative_words)\n",
    "    total_count = pos_count + neg_count\n",
    "    \n",
    "    if total_count == 0:\n",
    "        sentiment = \"neutral\"\n",
    "        score = 0.0\n",
    "    else:\n",
    "        score = (pos_count - neg_count) / total_count\n",
    "        if score > 0.1:\n",
    "            sentiment = \"positive\"\n",
    "        elif score < -0.1:\n",
    "            sentiment = \"negative\"\n",
    "        else:\n",
    "            sentiment = \"neutral\"\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'sentiment': sentiment,\n",
    "        'score': round(score, 2),\n",
    "        'positive_words': found_positive_words,\n",
    "        'negative_words': found_negative_words\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们创建一个同时具有我们两个自定义工具的代理，并测试它如何使用它们："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我可以帮您分析这两篇产品评论的情感和语言使用情况。我将使用工具来分析英文文本的情感和词汇使用情况，因此需要先将这些评论翻译成英文。\n",
      "\n",
      "首先，让我分析第一篇评论：\n",
      "Tool #3: text_sentiment_analyzer\n",
      "\n",
      "Tool #4: word_counter\n",
      "现在分析第二篇评论：\n",
      "Tool #5: text_sentiment_analyzer\n",
      "\n",
      "Tool #6: word_counter\n",
      "### 评论分析结果\n",
      "\n",
      "**评论1分析：**\n",
      "- **情感分析**：非常积极正面 (得分 1.0)\n",
      "- **积极词汇**：love (喜爱)、beautiful (漂亮)、best (最佳)\n",
      "- **没有负面词汇**\n",
      "- **语言特点**：\n",
      "  - 使用了强烈的肯定性词语，如\"绝对喜欢\"、\"完美\"、\"最佳\"\n",
      "  - 突出产品的多个优点：易用性、性能、设计美观\n",
      "  - 表达了个人满意度和推荐意愿\n",
      "  - 用词积极、热情\n",
      "\n",
      "**评论2分析：**\n",
      "- **情感分析**：非常消极负面 (得分 -1.0)\n",
      "- **负面词汇**：disappointing (令人失望)、terrible (糟糕)\n",
      "- **没有积极词汇**\n",
      "- **语言特点**：\n",
      "  - 使用了明确的负面评价词，如\"失望\"、\"糟糕\"\n",
      "  - 指出了具体问题：产品损坏、客服不佳\n",
      "  - 包含警告和劝阻其他消费者的语言\n",
      "  - 用词直接、批评性强\n",
      "\n",
      "**总结对比**：\n",
      "这两篇评论代表了产品评价的两个极端。第一篇充满热情和赞美，表达了高度满意；第二篇则表达了强烈的不满和失望，甚至试图劝阻他人购买。这种鲜明对比展示了客户体验可能的巨大差异，对于了解产品优缺点和改进方向具有重要参考价值。"
     ]
    }
   ],
   "source": [
    "text_analysis_agent = Agent(\n",
    "    model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    tools=[word_counter, text_sentiment_analyzer],\n",
    "    system_prompt=\"You are an assistant that specializes in text analysis.\"\n",
    ")\n",
    "\n",
    "response = text_analysis_agent(\"\"\"\n",
    "请分析这两篇产品评论，告诉我它们的情感和语言使用情况：\n",
    "\n",
    "评论1：\"我绝对喜欢这个产品！它使用简单，运行完美。设计很漂亮，这是我今年做出的最佳购买。\"\n",
    "\n",
    "评论2：\"这真是令人失望。产品到达时已经损坏，客服也毫无帮助。省下你的钱，避开这个糟糕的产品。\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools输入和输出类型\n",
    "\n",
    "Strands支持各种工具的输入和输出类型。让我们探索一些常见模式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def text_formatter(text: str, format_type: str, max_length: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    根据指定的格式选项格式化文本。\\\n",
    "参数：\n",
    "text (str)：要格式化的输入文本\n",
    "format_type (str)：要应用的格式化类型。有效选项有：\n",
    "'uppercase'、'lowercase'、'title_case'、'sentence_case'、'truncate'\n",
    "max_length (int, 可选)：使用'truncate'格式时的最大长度。默认为100。\n",
    "\n",
    "返回：\n",
    "str：格式化后的文本\n",
    "    \"\"\"\n",
    "    format_type = format_type.lower()\n",
    "    \n",
    "    if format_type == 'uppercase':\n",
    "        return text.upper()\n",
    "    elif format_type == 'lowercase':\n",
    "        return text.lower()\n",
    "    elif format_type == 'title_case':\n",
    "        return text.title()\n",
    "    elif format_type == 'sentence_case':\n",
    "        return '. '.join(s.capitalize() for s in text.split('. '))\n",
    "    elif format_type == 'truncate':\n",
    "        if len(text) <= max_length:\n",
    "            return text\n",
    "        return text[:max_length] + '...'\n",
    "    else:\n",
    "        return f\"Error: Unknown format type '{format_type}'.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理复杂数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "@tool\n",
    "def data_summarizer(data: List[Dict[str, Any]], fields_to_summarize: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    在字典列表（例如记录或JSON数据）中汇总数值字段。\n",
    "参数：\n",
    "data (List[Dict])：要汇总的字典（记录）列表\n",
    "fields_to_summarize (List[str])：要包含在汇总中的字段名称列表\n",
    "\n",
    "返回：\n",
    "Dict：包含每个指定字段汇总统计的字典：\n",
    "- count：记录数量\n",
    "- min：最小值\n",
    "- max：最大值\n",
    "- avg：平均值\n",
    "- sum：值的总和\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # Check if there's any data to summarize\n",
    "    if not data:\n",
    "        return {\"error\": \"未提供数据\"}\n",
    "    \n",
    "    # Process each requested field\n",
    "    for field in fields_to_summarize:\n",
    "        # Check if the field exists in the records\n",
    "        valid_values = []\n",
    "        for record in data:\n",
    "            if field in record and isinstance(record[field], (int, float)):\n",
    "                valid_values.append(record[field])\n",
    "        \n",
    "        # Generate statistics if we have valid values\n",
    "        if valid_values:\n",
    "            result[field] = {\n",
    "                \"count\": len(valid_values),\n",
    "                \"min\": min(valid_values),\n",
    "                \"max\": max(valid_values),\n",
    "                \"avg\": sum(valid_values) / len(valid_values),\n",
    "                \"sum\": sum(valid_values)\n",
    "            }\n",
    "        else:\n",
    "            result[field] = {\"error\": \"未找到有效值\"}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们用我们的新工具创建一个代理并测试它们："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我将帮您完成这两个任务：\n",
      "\n",
      "### 任务1：将文本格式化为标题格式\n",
      "\n",
      "我会使用text_formatter工具将您提供的文本转换为标题格式（title_case）。\n",
      "Tool #7: text_formatter\n",
      "### 任务2：总结销售数据\n",
      "\n",
      "我将使用data_summarizer工具来分析您提供的销售数据，并统计price、units_sold和rating字段。\n",
      "Tool #8: data_summarizer\n",
      "## 结果总结：\n",
      "\n",
      "1. **文本格式化结果**：\n",
      "   \"Strands Agents是一个强大的框架，用于构建具有高级功能的Ai助手。\"\n",
      "\n",
      "2. **销售数据统计**：\n",
      "\n",
      "   **价格(price)统计**:\n",
      "   - 记录数量: 5\n",
      "   - 最低价格: 200\n",
      "   - 最高价格: 1200\n",
      "   - 平均价格: 590.0\n",
      "   - 总价值: 2950\n",
      "\n",
      "   **销售数量(units_sold)统计**:\n",
      "   - 记录数量: 5\n",
      "   - 最低销售量: 45\n",
      "   - 最高销售量: 155\n",
      "   - 平均销售量: 94.0\n",
      "   - 总销售量: 470\n",
      "\n",
      "   **评分(rating)统计**:\n",
      "   - 记录数量: 5\n",
      "   - 最低评分: 4.2\n",
      "   - 最高评分: 4.8\n",
      "   - 平均评分: 4.56\n",
      "   - 总评分: 22.8"
     ]
    }
   ],
   "source": [
    "advanced_tools_agent = Agent(\n",
    "    model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    tools=[text_formatter, data_summarizer],\n",
    "    system_prompt=\"You are an assistant with advanced text and data processing capabilities.\"\n",
    ")\n",
    "\n",
    "response = advanced_tools_agent(\"\"\"\n",
    "我有几个任务给你：\n",
    "1. 将以下文本格式化为标题格式：\n",
    "\"strands agents是一个强大的框架，用于构建具有高级功能的AI助手。\"\n",
    "2. 总结这些销售数据：\n",
    "\n",
    "[\n",
    "{\"product\": \"笔记本电脑\", \"price\": 1200, \"units_sold\": 45, \"rating\": 4.7},\n",
    "{\"product\": \"手机\", \"price\": 800, \"units_sold\": 125, \"rating\": 4.5},\n",
    "{\"product\": \"平板电脑\", \"price\": 350, \"units_sold\": 85, \"rating\": 4.2},\n",
    "{\"product\": \"耳机\", \"price\": 200, \"units_sold\": 155, \"rating\": 4.8},\n",
    "{\"product\": \"显示器\", \"price\": 400, \"units_sold\": 60, \"rating\": 4.6}\n",
    "]\n",
    "\n",
    "对于销售数据，我想要关于price、units_sold和rating字段的统计数据。\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为自定义工具添加错误处理\n",
    "\n",
    "为生产级工具添加健全的错误处理至关重要。以下是一个具有适当错误处理的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def robust_text_analyzer(text: str, analysis_type: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    使用不同的分析方法分析文本，具有强大的错误处理功能。\n",
    "参数：\n",
    "text (str)：要分析的文本\n",
    "analysis_type (str)：要执行的分析类型。有效选项有：\n",
    "'word_count'、'char_count'、'sentence_count'、'readability'\n",
    "\n",
    "返回：\n",
    "dict：分析结果，如果出现错误则返回错误信息\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Input validation\n",
    "        if not isinstance(text, str):\n",
    "            return {\"error\": \"Input text must be a string\"}\n",
    "        \n",
    "        if not text.strip():\n",
    "            return {\"error\": \"Input text is empty\"}\n",
    "        \n",
    "        analysis_type = analysis_type.lower()  # Normalize input\n",
    "        valid_types = {'word_count', 'char_count', 'sentence_count', 'readability'}\n",
    "        \n",
    "        if analysis_type not in valid_types:\n",
    "            return {\n",
    "                \"error\": f\"Invalid analysis type: '{analysis_type}'.\", \n",
    "                \"valid_options\": list(valid_types)\n",
    "            }\n",
    "        \n",
    "        # Perform the requested analysis\n",
    "        if analysis_type == 'word_count':\n",
    "            words = text.split()\n",
    "            return {\n",
    "                \"total_words\": len(words),\n",
    "                \"unique_words\": len(set(words))\n",
    "            }\n",
    "            \n",
    "        elif analysis_type == 'char_count':\n",
    "            return {\n",
    "                \"total_chars\": len(text),\n",
    "                \"letters\": sum(c.isalpha() for c in text),\n",
    "                \"digits\": sum(c.isdigit() for c in text),\n",
    "                \"spaces\": sum(c.isspace() for c in text)\n",
    "            }\n",
    "            \n",
    "        elif analysis_type == 'sentence_count':\n",
    "            import re\n",
    "            sentences = re.split(r'[.!?](?:\\s|$)', text)\n",
    "            sentences = [s for s in sentences if s.strip()]\n",
    "            return {\n",
    "                \"sentence_count\": len(sentences),\n",
    "                \"avg_sentence_length\": len(text) / max(len(sentences), 1)\n",
    "            }\n",
    "            \n",
    "        elif analysis_type == 'readability':\n",
    "            words = text.split()\n",
    "            if not words:\n",
    "                return {\"error\": \"Cannot calculate readability for empty text\"}\n",
    "                \n",
    "            avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "            \n",
    "            if avg_word_length < 4:\n",
    "                difficulty = \"Easy\"\n",
    "            elif avg_word_length < 6:\n",
    "                difficulty = \"Medium\"\n",
    "            else:\n",
    "                difficulty = \"Complex\"\n",
    "                \n",
    "            return {\n",
    "                \"avg_word_length\": avg_word_length,\n",
    "                \"difficulty_estimate\": difficulty\n",
    "            }\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Catch and report any unexpected errors\n",
    "        return {\"error\": f\"Analysis failed with error: {str(e)}\"}\n",
    "\n",
    "# Create an agent with our robust tool\n",
    "robust_agent = Agent(\n",
    "    model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    tools=[robust_text_analyzer],\n",
    "    system_prompt=\"You are an assistant that can analyze text with high reliability.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 需要外部依赖的Tools\n",
    "\n",
    "自定义工具可以利用外部库提供高级功能。以下是一个使用NLTK进行文本处理的简化示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "# This is an example - you'd need to install NLTK first with:\n",
    "!pip install nltk\n",
    "\n",
    "@tool\n",
    "def nlp_tool(text: str, operation: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    使用NLP技术处理文本。\n",
    "参数：\n",
    "text: 要处理的文本\n",
    "operation: 要执行的NLP操作（'tokenize'、'pos_tag'、'entities'）\n",
    "\n",
    "返回：\n",
    "包含NLP处理结果的字典\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import nltk\n",
    "        # You would need to download nltk data packages first:\n",
    "        # nltk.download('punkt')\n",
    "        # nltk.download('averaged_perceptron_tagger')\n",
    "        \n",
    "        if operation == 'tokenize':\n",
    "            return {\n",
    "                \"tokens\": nltk.word_tokenize(text),\n",
    "                \"sentences\": nltk.sent_tokenize(text)\n",
    "            }\n",
    "        elif operation == 'pos_tag':\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            return {\"tagged\": nltk.pos_tag(tokens)}\n",
    "        else:\n",
    "            return {\"error\": f\"Unknown operation: {operation}\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# 注意：这只是一个例子，需要安装NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义工具的最佳实践\n",
    "\n",
    "创建Strands Agents的自定义工具时，请记住以下最佳实践：\n",
    "\n",
    "1. **编写清晰的文档字符串**：文档字符串是模型理解您工具用途和参数的方式。要详细且明确。\n",
    "2. **使用类型注解**：类型提示帮助模型理解您的工具期望的数据类型和返回值。\n",
    "3. **优雅地处理错误**：返回信息丰富的错误消息，而不是让异常传播。\n",
    "4. **保持工具专注**：每个工具应该把一件事做好，而不是尝试处理多个不相关的任务。\n",
    "5. **提供输入验证**：尽早验证输入，以防止处理无效数据。\n",
    "6. **使用描述性名称**：选择能清晰传达其用途的函数和参数名称。\n",
    "7. **返回结构化数据**：尽可能返回结构化数据（字典、列表），使模型易于处理。\n",
    "8. **考虑性能**：优化可能频繁调用或处理大量数据的工具。\n",
    "9. **彻底测试**：使用各种输入（包括边缘情况）测试您的工具，确保它们按预期运行。\n",
    "10. **版本依赖关系**：如果您的工具依赖外部库，请指定版本要求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 摘要\n",
    "\n",
    "在本章中，我们探讨了：\n",
    "- 使用`@tool`装饰器创建自定义工具\n",
    "- 清晰文档字符串和类型注解的重要性\n",
    "- 使用各种输入和输出类型\n",
    "- 为工具添加强大的错误处理\n",
    "- 将外部库集成到自定义工具中\n",
    "- 工具开发的最佳实践\n",
    "自定义工具是Strands Agents最强大的功能之一，允许您几乎向任何方向扩展代理的能力。通过创建设计良好的工具，您可以使您的代理执行复杂的、特定领域的任务，这些任务远远超出简单的文本生成。\n",
    "在下一章中，我们将探讨如何将Strands Agents与模型上下文协议(MCP)集成，这使得更强大的工具集成和互操作性成为可能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习\n",
    "\n",
    "1. 创建一个自定义工具，可以加载和解析CSV文件，然后对数据执行基本的数据分析操作。\n",
    "2. 构建一个连接天气API并返回给定位置天气预报的工具。\n",
    "3. 开发一个使用Pillow等库执行图像操作（如调整大小、格式转换）的工具。\n",
    "4. 创建一个与数据库交互以存储和检索信息的自定义工具。\n",
    "5. 设计一个验证和格式化JSON或XML等结构化数据的工具。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
