{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Customized Tools in Strands Agents\n",
    "\n",
    "## Introduction to Custom Tools\n",
    "\n",
    "While the built-in tools provide a solid foundation for many tasks, the real power of Strands Agents comes from the ability to create custom tools tailored to your specific needs. In this chapter, we'll explore how to create, refine, and extend your own tools to enhance your agents' capabilities.\n",
    "\n",
    "Custom tools allow you to:\n",
    "- Connect agents to your own APIs and services\n",
    "- Create domain-specific functionality\n",
    "- Build tools for specialized data processing\n",
    "- Interface with databases and external systems\n",
    "- Implement business logic specific to your use case\n",
    "\n",
    "Throughout this chapter, we'll use the Nova Lite model (`us.amazon.nova-lite-v1:0`) as specified for our course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Prerequisites\n",
    "\n",
    "Let's start by installing the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install strands-agents and strands-agents-tools if you haven't already\n",
    "!pip install -U strands-agents strands-agents-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Your First Custom Tool\n",
    "\n",
    "The simplest way to create a custom tool in Strands Agents is to use the `@tool` decorator with a Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent, tool\n",
    "\n",
    "# Define a simple custom tool\n",
    "@tool\n",
    "def word_counter(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Count the occurrence of each word in a text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary mapping words to their occurrence count\n",
    "    \"\"\"\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    import string\n",
    "    text = text.lower()\n",
    "    for punct in string.punctuation:\n",
    "        text = text.replace(punct, ' ')\n",
    "    \n",
    "    # Split into words and count occurrences\n",
    "    words = text.split()\n",
    "    word_counts = {}\n",
    "    for word in words:\n",
    "        word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    \n",
    "    return word_counts\n",
    "\n",
    "# Create an agent with our custom tool\n",
    "agent = Agent(\n",
    "    model=\"us.amazon.nova-lite-v1:0\",\n",
    "    tools=[word_counter],\n",
    "    system_prompt=\"You are an assistant that can analyze text and provide word statistics.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our custom tool by asking the agent to analyze some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent(\"\"\"\n",
    "Please analyze the following paragraph and tell me which words appear most frequently:\n",
    "\n",
    "\"Strands Agents is a powerful framework for building AI agents using Python. \n",
    "With Strands Agents, you can create intelligent agents that leverage language models \n",
    "like Nova Pro to perform complex tasks. The framework provides tools for enhancing \n",
    "your agents with various capabilities, making it easy to build sophisticated AI \n",
    "applications.\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of a Custom Tool\n",
    "\n",
    "Let's break down the key components of a custom tool in Strands Agents:\n",
    "\n",
    "1. **The `@tool` Decorator**: This tells Strands that the function should be treated as a tool\n",
    "2. **Type Annotations**: Used to define the expected input and output types\n",
    "3. **Docstring**: Crucial for the agent to understand when and how to use the tool\n",
    "4. **Implementation**: The actual code that executes when the tool is used\n",
    "\n",
    "### The Importance of Good Docstrings\n",
    "\n",
    "The docstring is particularly important because it helps the AI model understand:\n",
    "- What the tool does\n",
    "- When to use it\n",
    "- What inputs it expects\n",
    "- What outputs it produces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def text_sentiment_analyzer(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze the sentiment of a text using a simple lexicon-based approach.\n",
    "    \n",
    "    This tool classifies text sentiment by counting positive and negative words from a \n",
    "    predefined lexicon. It's useful for getting a quick sentiment assessment of user feedback,\n",
    "    product reviews, or social media comments.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to analyze for sentiment. Should be in English and ideally \n",
    "                    contain at least 10 words for better accuracy.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - 'sentiment': The overall sentiment ('positive', 'negative', or 'neutral')\n",
    "            - 'score': A sentiment score from -1.0 (very negative) to 1.0 (very positive)\n",
    "            - 'positive_words': List of positive words found in the text\n",
    "            - 'negative_words': List of negative words found in the text\n",
    "    \n",
    "    Note: This is a simplified sentiment analyzer for demonstration purposes.\n",
    "    \"\"\"\n",
    "    # Simple lexicons\n",
    "    positive_words = {\n",
    "        'good', 'great', 'excellent', 'wonderful', 'fantastic', 'amazing', 'love', 'best',\n",
    "        'happy', 'joy', 'positive', 'helpful', 'beautiful', 'perfect', 'recommend'\n",
    "    }\n",
    "    \n",
    "    negative_words = {\n",
    "        'bad', 'terrible', 'awful', 'horrible', 'poor', 'worst', 'hate', 'dislike', \n",
    "        'negative', 'disappointed', 'disappointing', 'useless', 'waste', 'broken'\n",
    "    }\n",
    "    \n",
    "    # Normalize text\n",
    "    text = text.lower()\n",
    "    words = ''.join(c if c.isalnum() else ' ' for c in text).split()\n",
    "    \n",
    "    # Find positive and negative words\n",
    "    found_positive_words = [word for word in words if word in positive_words]\n",
    "    found_negative_words = [word for word in words if word in negative_words]\n",
    "    \n",
    "    # Calculate sentiment score\n",
    "    pos_count = len(found_positive_words)\n",
    "    neg_count = len(found_negative_words)\n",
    "    total_count = pos_count + neg_count\n",
    "    \n",
    "    if total_count == 0:\n",
    "        sentiment = \"neutral\"\n",
    "        score = 0.0\n",
    "    else:\n",
    "        score = (pos_count - neg_count) / total_count\n",
    "        if score > 0.1:\n",
    "            sentiment = \"positive\"\n",
    "        elif score < -0.1:\n",
    "            sentiment = \"negative\"\n",
    "        else:\n",
    "            sentiment = \"neutral\"\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'sentiment': sentiment,\n",
    "        'score': round(score, 2),\n",
    "        'positive_words': found_positive_words,\n",
    "        'negative_words': found_negative_words\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create an agent with both of our custom tools and test how it uses them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_analysis_agent = Agent(\n",
    "    model=\"us.amazon.nova-lite-v1:0\",\n",
    "    tools=[word_counter, text_sentiment_analyzer],\n",
    "    system_prompt=\"You are an assistant that specializes in text analysis.\"\n",
    ")\n",
    "\n",
    "response = text_analysis_agent(\"\"\"\n",
    "Please analyze these two product reviews and tell me about their sentiment and language usage:\n",
    "\n",
    "Review 1: \"I absolutely love this product! It's easy to use and works perfectly. \n",
    "The design is beautiful and it's the best purchase I've made this year.\"\n",
    "\n",
    "Review 2: \"This was a terrible disappointment. The product arrived broken and \n",
    "customer service was unhelpful. Save your money and avoid this awful product.\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Input and Output Types\n",
    "\n",
    "Strands supports a variety of input and output types for tools. Let's explore some common patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def text_formatter(text: str, format_type: str, max_length: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Format text according to specified formatting options.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to format\n",
    "        format_type (str): The type of formatting to apply. Valid options are:\n",
    "                          'uppercase', 'lowercase', 'title_case', 'sentence_case', 'truncate'\n",
    "        max_length (int, optional): Maximum length when using 'truncate' format. Default is 100.\n",
    "        \n",
    "    Returns:\n",
    "        str: The formatted text\n",
    "    \"\"\"\n",
    "    format_type = format_type.lower()\n",
    "    \n",
    "    if format_type == 'uppercase':\n",
    "        return text.upper()\n",
    "    elif format_type == 'lowercase':\n",
    "        return text.lower()\n",
    "    elif format_type == 'title_case':\n",
    "        return text.title()\n",
    "    elif format_type == 'sentence_case':\n",
    "        return '. '.join(s.capitalize() for s in text.split('. '))\n",
    "    elif format_type == 'truncate':\n",
    "        if len(text) <= max_length:\n",
    "            return text\n",
    "        return text[:max_length] + '...'\n",
    "    else:\n",
    "        return f\"Error: Unknown format type '{format_type}'.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Complex Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "@tool\n",
    "def data_summarizer(data: List[Dict[str, Any]], fields_to_summarize: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Summarize numerical fields in a list of dictionaries (e.g., records or JSON data).\n",
    "    \n",
    "    Args:\n",
    "        data (List[Dict]): A list of dictionaries (records) to summarize\n",
    "        fields_to_summarize (List[str]): List of field names to include in the summary\n",
    "        \n",
    "    Returns:\n",
    "        Dict: A dictionary containing summary statistics for each specified field:\n",
    "              - count: Number of records\n",
    "              - min: Minimum value\n",
    "              - max: Maximum value\n",
    "              - avg: Average value\n",
    "              - sum: Sum of values\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # Check if there's any data to summarize\n",
    "    if not data:\n",
    "        return {\"error\": \"No data provided\"}\n",
    "    \n",
    "    # Process each requested field\n",
    "    for field in fields_to_summarize:\n",
    "        # Check if the field exists in the records\n",
    "        valid_values = []\n",
    "        for record in data:\n",
    "            if field in record and isinstance(record[field], (int, float)):\n",
    "                valid_values.append(record[field])\n",
    "        \n",
    "        # Generate statistics if we have valid values\n",
    "        if valid_values:\n",
    "            result[field] = {\n",
    "                \"count\": len(valid_values),\n",
    "                \"min\": min(valid_values),\n",
    "                \"max\": max(valid_values),\n",
    "                \"avg\": sum(valid_values) / len(valid_values),\n",
    "                \"sum\": sum(valid_values)\n",
    "            }\n",
    "        else:\n",
    "            result[field] = {\"error\": \"No valid values found\"}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an agent with our new tools and test them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_tools_agent = Agent(\n",
    "    model=\"us.amazon.nova-lite-v1:0\",\n",
    "    tools=[text_formatter, data_summarizer],\n",
    "    system_prompt=\"You are an assistant with advanced text and data processing capabilities.\"\n",
    ")\n",
    "\n",
    "response = advanced_tools_agent(\"\"\"\n",
    "I have a couple tasks for you:\n",
    "\n",
    "1. Format the following text in title case:\n",
    "\"strands agents is a powerful framework for building AI assistants with advanced capabilities.\"\n",
    "\n",
    "2. Summarize this sales data:\n",
    "[\n",
    "    {\"product\": \"Laptop\", \"price\": 1200, \"units_sold\": 45, \"rating\": 4.7},\n",
    "    {\"product\": \"Phone\", \"price\": 800, \"units_sold\": 125, \"rating\": 4.5},\n",
    "    {\"product\": \"Tablet\", \"price\": 350, \"units_sold\": 85, \"rating\": 4.2},\n",
    "    {\"product\": \"Headphones\", \"price\": 200, \"units_sold\": 155, \"rating\": 4.8},\n",
    "    {\"product\": \"Monitor\", \"price\": 400, \"units_sold\": 60, \"rating\": 4.6}\n",
    "]\n",
    "\n",
    "For the sales data, I want statistics on price, units_sold, and rating fields.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Error Handling to Custom Tools\n",
    "\n",
    "Robust error handling is crucial for production-grade tools. Here's an example with proper error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def robust_text_analyzer(text: str, analysis_type: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze text using different analysis methods with robust error handling.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to analyze\n",
    "        analysis_type (str): The type of analysis to perform. Valid options are:\n",
    "                            'word_count', 'char_count', 'sentence_count', 'readability'\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results of the analysis, or an error message if something went wrong\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Input validation\n",
    "        if not isinstance(text, str):\n",
    "            return {\"error\": \"Input text must be a string\"}\n",
    "        \n",
    "        if not text.strip():\n",
    "            return {\"error\": \"Input text is empty\"}\n",
    "        \n",
    "        analysis_type = analysis_type.lower()  # Normalize input\n",
    "        valid_types = {'word_count', 'char_count', 'sentence_count', 'readability'}\n",
    "        \n",
    "        if analysis_type not in valid_types:\n",
    "            return {\n",
    "                \"error\": f\"Invalid analysis type: '{analysis_type}'.\", \n",
    "                \"valid_options\": list(valid_types)\n",
    "            }\n",
    "        \n",
    "        # Perform the requested analysis\n",
    "        if analysis_type == 'word_count':\n",
    "            words = text.split()\n",
    "            return {\n",
    "                \"total_words\": len(words),\n",
    "                \"unique_words\": len(set(words))\n",
    "            }\n",
    "            \n",
    "        elif analysis_type == 'char_count':\n",
    "            return {\n",
    "                \"total_chars\": len(text),\n",
    "                \"letters\": sum(c.isalpha() for c in text),\n",
    "                \"digits\": sum(c.isdigit() for c in text),\n",
    "                \"spaces\": sum(c.isspace() for c in text)\n",
    "            }\n",
    "            \n",
    "        elif analysis_type == 'sentence_count':\n",
    "            import re\n",
    "            sentences = re.split(r'[.!?](?:\\s|$)', text)\n",
    "            sentences = [s for s in sentences if s.strip()]\n",
    "            return {\n",
    "                \"sentence_count\": len(sentences),\n",
    "                \"avg_sentence_length\": len(text) / max(len(sentences), 1)\n",
    "            }\n",
    "            \n",
    "        elif analysis_type == 'readability':\n",
    "            words = text.split()\n",
    "            if not words:\n",
    "                return {\"error\": \"Cannot calculate readability for empty text\"}\n",
    "                \n",
    "            avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "            \n",
    "            if avg_word_length < 4:\n",
    "                difficulty = \"Easy\"\n",
    "            elif avg_word_length < 6:\n",
    "                difficulty = \"Medium\"\n",
    "            else:\n",
    "                difficulty = \"Complex\"\n",
    "                \n",
    "            return {\n",
    "                \"avg_word_length\": avg_word_length,\n",
    "                \"difficulty_estimate\": difficulty\n",
    "            }\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Catch and report any unexpected errors\n",
    "        return {\"error\": f\"Analysis failed with error: {str(e)}\"}\n",
    "\n",
    "# Create an agent with our robust tool\n",
    "robust_agent = Agent(\n",
    "    model=\"us.amazon.nova-lite-v1:0\",\n",
    "    tools=[robust_text_analyzer],\n",
    "    system_prompt=\"You are an assistant that can analyze text with high reliability.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools with External Dependencies\n",
    "\n",
    "Custom tools can leverage external libraries to provide advanced functionality. Here's a simplified example that would use NLTK for text processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example - you'd need to install NLTK first with:\n",
    "# !pip install nltk\n",
    "\n",
    "@tool\n",
    "def nlp_tool(text: str, operation: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process text using NLP techniques.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to process\n",
    "        operation: The NLP operation to perform ('tokenize', 'pos_tag', 'entities')\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing the results of the NLP processing\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import nltk\n",
    "        # You would need to download nltk data packages first:\n",
    "        # nltk.download('punkt')\n",
    "        # nltk.download('averaged_perceptron_tagger')\n",
    "        \n",
    "        if operation == 'tokenize':\n",
    "            return {\n",
    "                \"tokens\": nltk.word_tokenize(text),\n",
    "                \"sentences\": nltk.sent_tokenize(text)\n",
    "            }\n",
    "        elif operation == 'pos_tag':\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            return {\"tagged\": nltk.pos_tag(tokens)}\n",
    "        else:\n",
    "            return {\"error\": f\"Unknown operation: {operation}\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Note: This is just an example and would require NLTK to be installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Custom Tools\n",
    "\n",
    "When creating custom tools for Strands Agents, keep these best practices in mind:\n",
    "\n",
    "1. **Write Clear Docstrings**: The docstring is how the model understands your tool's purpose and parameters. Be detailed and explicit.\n",
    "2. **Use Type Annotations**: Type hints help the model understand what data types your tool expects and returns.\n",
    "3. **Handle Errors Gracefully**: Return informative error messages instead of allowing exceptions to propagate.\n",
    "4. **Keep Tools Focused**: Each tool should do one thing well rather than trying to handle multiple unrelated tasks.\n",
    "5. **Provide Input Validation**: Validate inputs early to prevent processing invalid data.\n",
    "6. **Use Descriptive Names**: Choose function and parameter names that clearly convey their purpose.\n",
    "7. **Return Structured Data**: When possible, return structured data (dictionaries, lists) that's easy for the model to work with.\n",
    "8. **Consider Performance**: Optimize tools that might be called frequently or process large amounts of data.\n",
    "9. **Test Thoroughly**: Test your tools with various inputs, including edge cases, to ensure they behave as expected.\n",
    "10. **Version Dependencies**: If your tool depends on external libraries, specify version requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this chapter, we've explored:\n",
    "\n",
    "- Creating custom tools using the `@tool` decorator\n",
    "- The importance of clear docstrings and type annotations\n",
    "- Working with various input and output types\n",
    "- Adding robust error handling to tools\n",
    "- Integrating external libraries into custom tools\n",
    "- Best practices for tool development\n",
    "\n",
    "Custom tools are one of the most powerful features of Strands Agents, allowing you to extend your agents' capabilities in virtually any direction. By creating well-designed tools, you can enable your agents to perform complex, domain-specific tasks that go far beyond simple text generation.\n",
    "\n",
    "In the next chapter, we'll explore how to integrate Strands Agents with the Model Context Protocol (MCP), which enables even more powerful tool integration and interoperability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Create a custom tool that can load and parse CSV files, then perform basic data analysis operations on the data.\n",
    "2. Build a tool that connects to a weather API and returns weather forecasts for a given location.\n",
    "3. Develop a tool that performs image operations (e.g., resizing, format conversion) using a library like Pillow.\n",
    "4. Create a custom tool that interacts with a database to store and retrieve information.\n",
    "5. Design a tool that validates and formats structured data like JSON or XML."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
